{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how AllenNlp handle one of the key aspects of applying deep learning techniques to textual data: learning distributed representations of words and sentences.\n",
    "\n",
    "Recently, there has been an explosion of different techniques to represent words and sentences in NLP, including pre-trained word vectors, character level CNN encodings and sub-word token representation (e.g byte encodings). Going even further, using learned representations of higher level lingustic features, such as POS tags, named entities and dependency paths has proved successful.\n",
    "\n",
    "In order to deal with this breadth of methods for representing words as vectors, AllenNLP introduces 3 key abstractions:\n",
    "\n",
    "- `TokenIndexers`, which generate indexed tensors representing sentences in different ways. See the `data_pipeline` for more info. \n",
    "\n",
    "- `TokenEmbedders`, which transform indexed tensors into embedded representations. At its most basic, this is just a standard `Embedding` layer you'd find in any neural network library. However, they can be more complex - for instance, AllenNLP has a `token_characters_encoder` which applies a CNN to character level representations.\n",
    "- `TextFieldEmbedders`, which are a wrapper around a set of `TokenEmbedders`. At it's most basic, this applies the `TokenEmbedders` which it is passed and concatenates their output.\n",
    "\n",
    "Using this hierarchy allows you to easily compose different representations of a sentence together in modular ways. For instance, in the Bidaf model, we use this to concatenate a character level CNN encoding of the words in the sentence to the pretrained word embeddings. You can also specify this completely from a JSON file, making experimenation with different representations extremely easy.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just makes sure the library paths are correct. \n",
    "# You need to run this cell before you run the rest of this\n",
    "# tutorial, but you can ignore the contents!\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "\n",
    "sentence = TextField([\"All\", \"the\", \"cool\", \"kids\", \"use\", \"character\", \"embeddings\", \".\"],\n",
    "                     token_indexers={\"tokens\": SingleIdTokenIndexer(),\n",
    "                                     \"characters\": TokenCharactersIndexer()})\n",
    "sentence_instance = Instance({\"sentence\": sentence})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a small vocabulary from our sentence - note that because we have used both a\n",
    "`SingleIdTokenIndexer` and a `TokenCharactersIndexer`, when we call `Vocabulary.from_dataset`, the created `Vocabulary` will have two namespaces, which correspond to the keys in this token indexer dictionary in our `TextField`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Vocabulary, Dataset\n",
    "\n",
    "vocab = Vocabulary.from_dataset(Dataset([sentence_instance]))\n",
    "\n",
    "print(\"This is the token vocabulary we created: \\n\")\n",
    "print(vocab.get_index_to_token_vocabulary(\"tokens\"))\n",
    "\n",
    "print(\"This is the character vocabulary we created: \\n\")\n",
    "print(vocab.get_index_to_token_vocabulary(\"characters\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

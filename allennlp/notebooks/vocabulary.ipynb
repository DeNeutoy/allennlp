{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Vocabularies in AllenNLP\n",
    "\n",
    "Before we start, this tutorial assumes you've already gone through the tutorial on `Datasets`, `Instances` and `Fields`. If you haven't, you might want to check out that one first as we make use of some of these constructs to explain the `Vocabulary` functionality.\n",
    "\n",
    "A `Vocabulary` maps strings to integers, allowing for strings to be mapped to an\n",
    " out-of-vocabulary token.\n",
    "\n",
    "Vocabularies can be fit to a particular dataset, which we use to decide which tokens are\n",
    " in-vocabulary, or alternatively, they can be loaded directly from a static vocabulary file.\n",
    "\n",
    "\n",
    "First, let's import the vocabulary class from `allennlp` and create a vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell just makes sure the library paths are correct. \n",
    "# You need to run this cell before you run the rest of this\n",
    "# tutorial, but you can ignore the contents!\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.data import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's create an empty `Vocabulary` so we can look at the arguments it takes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocabulary(counter=None, min_count=1, max_vocab_size=100000, non_padded_namespaces=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The vocabulary takes 4 arguments: \n",
    "\n",
    "- A counter, which is a `Dict[str, Dict[str, int]]`: This is a nested dictionary because the allennlp Vocabulary class supports the idea of \"namespaces\". A namespace is a vocabulary which is associated with a part of your data. For instance, in a sequence tagging model, you would typically have two namespaces: A namespace of words for your textual input and a namespace of tags(e.g. \"NP\", \"VP\", etc) for your labels. This counter is therefore a mapping from string namespace names to their respective mapping dictionaries of `Dict[tokens => counts]`.\n",
    "\n",
    "\n",
    "- A minimum count: Tokens with smaller counts than this won't be included in your `Vocabulary`.\n",
    "\n",
    "\n",
    "- A maximum vocab size: The lowest frequency words will be dropped to make your vocabulary this size.\n",
    "\n",
    "\n",
    "- Non padded namespaces: For some namespaces, such as words, we provide additional tokens commonly used in NLP applications - specifically, \"@@PADDING@@\" and \"@@UNKNOWN@@\". Why did we use these weird tokens we hear you ask? Well, if anything goes wrong in your model, it's going to be pretty obvious, because these tokens are pretty hard to miss. However, for other namespaces, such as tags, you _don't_ want these extra tokens, because in your model, you are going to be creating a distribution over the size of this namespace, so if we have added extra tags, your model could predict these. Naturally, we don't want this to happen, so we provide some reasonable defaults: any vocabulary namespace ending with `tag` or `label` won't have these extra tokens by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It's easy to interact with the vocabulary we just created. Let's add some words!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '@@PADDING@@', 1: '@@UNKOWN@@', 2: 'Paul', 3: 'Allen'}\n",
      "{0: 'PERSON', 1: 'PLACE'}\n"
     ]
    }
   ],
   "source": [
    "vocab.add_token_to_namespace(\"Paul\", namespace=\"tokens\")\n",
    "vocab.add_token_to_namespace(\"Allen\", namespace=\"tokens\")\n",
    "\n",
    "vocab.add_token_to_namespace(\"PERSON\", namespace=\"tags\")\n",
    "vocab.add_token_to_namespace(\"PLACE\", namespace=\"tags\")\n",
    "print(vocab.get_index_to_token_vocabulary(\"tokens\"))\n",
    "print(vocab.get_index_to_token_vocabulary(\"tags\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we print the namespace for `tags` we don't have any padding tokens or unknown tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above, we demonstrated the basic functionality of the namespaces in the Vocabulary. However, we'd ideally like to \n",
    "generate a full `Vocabulary` without having to individually add all the different words. Below, we'll generate a `Dataset` consisting of a single `Instance` and use it to automatically generate a `Vocabulary`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, TagField\n",
    "from allennlp.data import Dataset, Instance\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "sentence = TextField([\"Paul\", \"Allen\", \"is\", \"a\", \"great\", \"guy\", \".\"], token_indexers=[SingleIdTokenIndexer()])\n",
    "tags = TagField([\"PERSON\", \"PERSON\", \"O\", \"O\", \"O\", \"O\", \"O\"], sentence, tag_namespace=\"tags\")\n",
    "toy_dataset = Dataset([Instance({\"sentence\": sentence, \"tags\": tags})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Now we've generated this baby dataset with one training instance, we can generate a `Vocabulary` using a classmethod on `Vocabulary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5184.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '@@PADDING@@', 1: '@@UNKOWN@@', 2: 'a', 3: 'great', 4: 'Allen', 5: '.', 6: 'Paul', 7: 'guy', 8: 'is'}\n",
      "{0: 'O', 1: 'PERSON'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_dataset(toy_dataset)\n",
    "print(vocab.get_index_to_token_vocabulary(\"tokens\"))\n",
    "print(vocab.get_index_to_token_vocabulary(\"tags\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}